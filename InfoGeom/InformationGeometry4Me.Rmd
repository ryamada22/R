---
title: "私のための情報幾何 InformationGeometry4Me"
author: "ryamada"
date: "2016年12月2日"
output: 
  html_document:
    toc: true
    toc_depth: 6
    number_section: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 結論 Conclusion

尤度関数 $p(w,\xi)$ (ただし、$w$は事象、$\xi$はパラメタ(取り方に寄らず、一般的な意味での))には、２つのよいパラメタの取り方$\theta$ と $\eta$がある。

$$
\log{p(w)} = C(w) + F \theta - \phi(\theta)
$$
$$
p(w) = F^{-1}(w) \eta
$$

この二つのパラメタの取り方はには２つの特徴がある。

一つ目の特徴は$\theta$, $\eta$のそれぞれに備わったものであり、もう一つの特徴は、$\theta$と$\eta$との相互関係に関するものである。２つのパラメタの取り方が相互にしていることは、上の２つの式が同一の行列$F$を持っていることから解る。

+ $\theta$, $\eta$のパラメタの取り方をすると、どちらも「平坦」という性質を持つ。「平坦」である、とは、直線が、パラメタの一次線形式で表せるという性質のことである。

+ $\theta$, $\eta$ 座標系は相互に「直交」するという性質を持つ。「直交」するというのは角度が90度である、というよりは、距離を測るときにピタゴラスの定理のようなものを使える、という意味である。その具体的な意味は後述する。

# どのように役に立てるか How it works for me

尤度関数が空間に置かれている(多様体をなしている)。

その多様体上で、個々の尤度関数同士がどれくらい離れているかを測る(尤度比を知るために、最尤推定をするために)りたい。

そのときに、まっすぐな道を通ることが最短距離である性質(平坦)と、ピタゴラスの定理を使えるという性質が便利である。

実際には、$\eta$座標の一部を固定し、残りを自由にすると、すべての$eta$座標が自由な空間から、一部が制約された部分空間への垂線を取るという作業が発生する。

このとき、$\eta$,$\theta$座標系が相互に直交しているので、$\theta$座標の軸に沿った直線が垂線になるという性質があり、垂線とその足を見出すことが容易になる。

# 一目で見る基本概念 Basic concepts @ a glance

尤度関数と情報幾何では、平坦・直線・距離・ピタゴラスの定理を扱うが、ユークリッド幾何的なそれを拡張した上で使っているので、拡張された意味を理解することが必要である。

## 平坦

直線が座標系によって一次関数で表されることを言う。

### 接続
平坦か平坦でないかを定量化したものに「接続」と呼ばれる係数がある。
接ベクトルがある接ベクトル方向に動いたときにまったく変わらなければ、平坦だが、少し変わるとすると、その変わり具合を表現する必要がある。

その変わり具合は色々な方向に起きた変わり具合を併せたものとなる。

結局、$n$個のパラメタで表されている場合には、$n$個の接ベクトルが、$n$個の接ベクトル方向についてどのように変化するかが必要であるから$n^2$ペアの評価が必要となる。

そして、$n^2$のそれぞれは$n$方向成分に分解されるので、$n^2 \times n = n^3$の値の組が、接ベクトルの変化府愛を説明する。

平坦というのは、この$n^3$の係数がすべて0であるような状態のこと、そのようなパラメタの取り方のことである。

### 直線と測地線

平坦な座標系でパラメタの一次線形式で表されるものが直線である。

座標系の取り方を変えると、直線が曲がって見える。

この曲がって見える線が、「最短距離」を表しているときには、「直線」的な意味を持つので、そのような曲線を「測地線」と呼ぶ。


## 距離

### 内積と距離

ユークリッド空間における、2点x=(x_i) と y=(y_i)との距離は

$$
d_{xy}=\sqrt{\sum (x_i-y_i)^2}
$$

と計算される。

$$
d_{xy}^2 = \sum (x_i-y_i)^2 = \sum x_i^2 + \sum y_i^2 - 2 \sum x_i y_i
$$

と書き直し、さらに$\sum x_i y_i$を$x$と$y$との普通の意味での内積であると読めば

$$
d_{xy}^2 = ||x||^2 + ||y||2 - 2 (x,y)
$$

と書ける。さらに$||x||^2$も$x$と$x$自身の内積だと考えれば

$$
d_{xy}^2 = (x,x) + (y,y) - 2(x,y)
$$
となる。

そしてこれを「距離」の定義だと考えることもできる。

普通の意味での内積というのは、ベクトルのペアに関して

$$
(x,y) = x^T G y
$$
と言うベクトルと行列の演算をしているとみなした場合、$G$が単位行列になっていることに相当する。

ユークリッド空間では内積が単位行列で定義されている、と言い換えて、ユークリッド空間でない場合には、内積が単位行列とは異なる正方行列$G$で定義されている、と見ることができる。

この$G$を多様体のリーマン計量と呼ぶ。
また、情報幾何ではフィッシャー情報行列がこのリーマン計量と同じである。

### 距離の拡張としてのKL divergence

距離は内積を使って
$$
d_{xy}^2 = (x,x) + (y,y) - 2(x,y)
$$
と表された。

実際には

ここでの$x$と$y$とは同じ座標系で表されていたのだが、情報幾何では、相互によい関係にある２つの座標系を用いる。

$x$を一つ目の座標系で表し、$y$をもう一つの座標系で表したものとしたときに、一つ目の座標系での$x$と$x$との内積があり、もう一つの座標系で$y$と$y$との内積があり、異なる座標系で表された$x$,$y$とについても$(x,y)$が定義されていれば、上式は計算できる。

実際、

$$
\frac{1}{2} d_{xy}^2 = \frac{1}{2} (x,x) + \frac{1}{2} (y,y) - (x,y) = KLd(x||y)
$$
がKL divergenceと呼ばれる量である。

$(x,x)$,$(y,y)$ は平坦な座標系でのそれなので、「まっすぐ」に測ることができる。

ユークリッド空間で$x$と$y$とが直交していれば、内積$(x,y)$は0になって、

$$
\frac{1}{2} d_{xy}^2 = \frac{1}{2} (x,x) + \frac{1}{2} (y,y)
$$
となるが、これは「ピタゴラスの定理」の性質である。

$\theta$,$\eta$座標系において、両者が直交しているとき、$\theta$座標系での直線$x$と$\eta$座標系での直線$y$とは相互に直交しているから、この式が成り立つ。

それを
$$
KLd(p||r) = KLd(p||q) + KLd(q||r)
$$
と書く。ただし、p-qは$\theta$座標系での直線、q-rは$\eta$座標系での直線である。

## 曲がっていることとリーマン計量と接続

ユークリッド空間では、リーマン計量が単位行列であって、接続係数がすべて0であった。

曲がっている多様体ではリーマン計量が単位行列ではないし、接続係数も0ではない。

逆に言うと、座標系を入れて多様体を考えたり、多様体上の「距離」を測ったりするときには、リーマン計量と接続との両方を考える必要がある。

パラメタの数が$n$個のとき、リーマン計量は$n \times n$行列の形をしているし、接続係数は$n^3$個の値の組になっている。

## 双対

二つの座標系$\theta$,$\eta$が相互にうまい関係になっている話をしている。

この２つは双対とよばれる関係である。お互い様、である。

そのお互い様がどうして、$\log{p(w)}$と$p(w)$と関係するのか、と言ったことを確認するのが、以降の「解説」の記述である。

$$
\log{p(w)} = C(w) + F \theta - \phi(\theta)
$$
$$
p(w) = F^{-1}(w) \eta
$$

# 解説 Details

## 確率密度関数と尤度関数

確率密度関数(確率質量関数)は、パラメタを確率変数の値との関数で

$$
p(w,\xi)
$$
と書ける。

これは尤度関数でもある。

以降は「尤度関数」とよぶことにする。

## 対数尤度関数

$$
L(w,\xi) = \log{p(w,\xi)}
$$
をよく使う。
理由もあるが、それは後述する。

## 尤度関数・対数尤度関数の微分

以下の変換を多用するので、列挙しておく。

$$
L(w,\xi) = \log{p(w,\xi)}
$$
$$
\frac{\partial}{\partial \xi_i} L(w,\xi) = \frac{1}{p(w,\xi)} \frac{\partial}{\partial \xi_i} p(w,\xi)
$$

$$
p(w,\xi) = \frac{\frac{\partial p(w,\xi)}{\partial \xi_i}}{\frac{\partial L(w,\xi)}{\partial \xi_i}}
$$
$$
\frac{\partial}{\partial \xi_i} p(w,\xi) = \frac{\partial L(w,\xi)}{\partial \xi_i} p(w,\xi)
$$


## 対数尤度関数とスコア関数

パラメタ$\xi$の最尤推定は、$\frac{\partial p(w,\xi)}{\partial \xi_i}=0$ を満足する$\xi_i$であるが、$\frac{\partial L(w,\xi)}{\partial \xi_i}=0$ でもよい。

対数尤度関数の方が有益なのは、

$$
E[\frac{\partial L(w,\xi)}{\partial \xi_i}] = 0
$$

であるから。
期待値が0というのは、多数回行ったときに、標本平均を取ることが適切であることを意味するから。

ちなみに$E[\frac{\partial p(w,\xi)}{\partial \xi_i} \ne 0]$ 。

念のため$E[\frac{\partial L(w,\xi)}{\partial \xi_i}] = 0$を確かめる。

$$
E[\frac{\partial L(w,\xi)}{\partial \xi_i}] = \int_\Omega \frac{\partial L(w,\xi)}{\partial \xi_i} p(w,\xi) d\mu = \int_\Omega \frac{1}{p(w,\xi)} \frac{\partial p(w,\xi)}{\partial \xi_i} p(w,\xi) d\mu = \int_\Omega \frac{\partial p(w,\xi)}{\partial \xi_i} d\mu = 0
$$
### スコア関数

このように対数尤度関数の方が尤度関数そのものよりよい性質を持っているので、対数尤度関数の微分には名前がついている。

それがスコア関数である。

$$
\frac{\partial L(w,\xi)}{\partial \xi_i}
$$

## フィッシャー情報量

スコア関数が0のところが最尤推定値である。

最尤推定値が効率よく見つかる条件というのは、スコア関数の値が0の値の周辺で大きく変化していることである。

期待値が0の周りで変化が大きいことは分散が大きいことに相当するので。スコア関数の分散が、0回りで大きいか小さいかを値にすることは有用であると解る。

偏微分をしているので、「2乗」というよりは、分散・共分散成分を知ることが大事であるから、
その期待値としては

$$
E[\frac{\partial L(w,\xi)}{\partial \xi_i} \frac{\partial L(w,\xi)}{\partial \xi_j}]
$$

となる。

これを
$$
g_{ij} = E[\frac{\partial L(w,\xi)}{\partial \xi_i} \frac{\partial L(w,\xi)}{\partial \xi_j}]
$$
と表す。



## フィッシャー情報量とリーマン計量の一致

前節で示したようにフィッシャー情報量は正方行列で

$$
g_{ij}=E[\frac{\partial L(w,\xi)}{\partial \xi_i} \frac{\partial L(w,\xi)}{\partial \xi_j}] = \int_\Omega \frac{\partial L(w,\xi)}{\partial \xi_i} \frac{\partial L(w,\xi)}{\partial \xi_j} p(w,\xi)d \mu
$$

一方、

$$
E[\frac{\partial}{\partial \xi_i} \frac{\partial}{\partial \xi_j} L(w,\xi)] = E[\frac{\partial}{\partial \xi_i}(\frac{1}{p(w,\xi)}\frac{\partial p(w,\xi)}{\partial \xi_j})]

$$
さらに変形して

$$
= E[-\frac{1}{p(w,\xi)^2}\frac{\partial p(w,\xi)}{\partial \xi_i})\frac{\partial p(w,\xi)}{\partial \xi_j})] + E[\frac{1}{p(w,\xi)}\frac{\partial}{\partial \xi_i}\frac{\partial p(w,\xi)}{\partial \xi_j}]
$$
第1項は
$$

- \int_\Omega (\frac{1}{p(w,\xi)} \frac{\partial p(w,\xi)}{\partial \xi_i}) (\frac{1}{p(w,\xi)} \frac{\partial p(w,\xi)}{\partial \xi_j}) d \mu
= - \int_\Omega \frac{\partial L(w,\xi)}{\partial \xi_i}\frac{\partial L(w,\xi)}{\partial \xi_j} d \mu
$$

第2項は
$$
\int_\Omega \frac{1}{p(w,\xi)}\frac{\partial}{\partial \xi_i}\frac{\partial p(w,\xi)}{\partial \xi_j} p(w,\xi) d \mu = \int_\Omega \frac{\partial}{\partial \xi_i}\frac{\partial p(w,\xi)}{\partial \xi_j} d\mu = 0
$$

なので、結局、

$$
E[\frac{\partial}{\partial \xi_i} \frac{\partial}{\partial \xi_j} L(w,\xi)] = - \int_\Omega \frac{\partial L(w,\xi)}{\partial \xi_i}\frac{\partial L(w,\xi)}{\partial \xi_j} d \mu
$$
つまり、フィッシャー情報量は、対数尤度関数の2階の微分に(-1)を掛けたものになっていることが解る。

2階の微分というのは、曲がり具合〜曲率であるから、この行列は多様体の曲率に相当する量であり、リーマン計量のことである。

ここで、推定のよさであるフィッシャー情報行列がリーマン計量であることがわかった。

### リーマン計量と「内積」

リーマン計量というのは、局所にベクトルがあったときに、それが作る内積をどのくらいの大きさにするかを決める行列である。

いわゆる内積を返すようなリーマン計量が単位行列であり、曲がっていれば、単位行列ではなくなることや、曲がっているところの「長さ」を測るときに用いられた行列$G$がこのリーマン計量である。


## $p(w,\xi)$ と $\log{p(w,\xi)}$ との一般化

関数とその対数とを「一般化」する必要などあるとも感じられないが、この２つを取ることの意味・意義について腑に落ちるためには、この「一般化」について知っておくことは有用である。

フィッシャー情報量を別の形で表現する、ということをとっかかりに、この「一般化」を眺めてみることにする。

### フィッシャー情報量と球面と自然な計量

フィッシャー情報量は以下のようにも表せる。

$$
g_{ij}= \int_\Omega \frac{\partial L(w,\xi)}{\partial \xi_i} \frac{\partial L(w,\xi)}{\partial \xi_j} p(w,\xi)d \mu = \int_\Omega \frac{1}{p(w,\xi)}\frac{\partial p(w,\xi)}{\partial \xi_i}\frac{\partial p(w,\xi)}{\partial \xi_j} d \mu
$$ 

この式では

$$
\frac{\partial L(w,\xi)}{\partial \xi_i} p(w,\xi) = \frac{1} {p(w,\xi)}\frac{\partial p(w,\xi)}{\partial \xi_i} p(w,\xi)
$$


とすることで、分母分子に現れる$p(w,\xi)$をキャンセル・アウトできる。

今、二つの$\frac{\partial L(w,\xi)}{\partial \xi_i}$ $\frac{\partial L(w,\xi)}{\partial \xi_j}$を不平等に扱って$p(w,\xi)$をキャンセルアウトする代わりに、２つを平等に使ってキャンセル・アウトすることを考える。


そのように考えると、以下も成り立つことが示せる。


$$
g_{ij} = \int_\Omega \frac{\partial 2\sqrt{p(w,\xi)}}{\partial \xi_i}\frac{\partial 2\sqrt{p(w,\xi)}}{\partial \xi_j} d \mu= \int_\Omega \frac{1}{p(w,\xi)}\frac{\partial p(w,\xi)}{\partial \xi_i}\frac{\partial p(w,\xi)}{\partial \xi_j} d \mu
$$
 
これは、$2\sqrt{p(w,\xi)}$という変換をすると、半径２の球面になって、そこでの計量は一定になる、ということに相当する。

$\Omega$が有限個の状態であるときには、その個数次元の球面を考えればよい。

### 不均等なペアに分解する

均等に分解することも可能だが、$p(w,\xi)$をキャンセル・アウトするだけであれば、色々な不均等分解が可能である。

次のような不均等分解が可能である。

$$
f^{(k)}(w,\xi)= k p(w,\xi)^\frac{1}{k}
$$

とおけば

$$
g_{ij} = \int_\Omega \frac{\partial f^{(k)}(w,\xi)}{\partial \xi_i}\frac{\partial f^{(\frac{1}{k})}(w,\xi)}{\partial \xi_i} d \mu
$$

が、そのような不均等分解であることが確かめられる。

ちなみに、このような$k$を用いた関数ではなく

$$
f^{(\alpha)}(w,\xi)= \frac{2}{1-\alpha} p(w,\xi)^\frac{1-\alpha}{2}
$$
と表現するのが通例である。

このような$\alpha$での表現にすることで、

$\alpha=0$のときに

$$
f^{(\alpha=0)}(w,\xi) = 2 p(w,\xi)^{\frac{1}{2}}
$$

となり、これは、「均等分解」のときの表現となる。


そして、
$$
f^{\alpha=-1}(w,\xi) = p(w,\xi)
$$

なわけであるが、このときのペアはもともと$\log(p(w,\xi))$であったが、
$$
f^{(\alpha=1)}(w,\xi) =\frac{2}{1-1} p(w,\xi)^{\frac{1-1}{2}}
$$
となって、計算できない式になる。
逆に
$$
f^{(\alpha=1)} = \log{p(w,\xi)}
$$
と定義することで、首尾一貫する。

この$f^{(\alpha)}(w,\xi)$と$f^{(-\alpha)}(w,xi)$とは、お互いに補い合って、フィッシャー情報量(リーマン計量)を分解する関係であり、双対になっているという。

### 平坦と直交と$\alpha= \pm 1$

$f^{(\alpha = \pm 1)}(w,\xi)$は、ある特別なパラメタの取り方をすると、「平坦」になることが知られている。

そのパラメタの取り方が

$$
\log{p(w)} = C(w) + F \theta - \phi(\theta)
$$
$$
p(w) = H(w) \eta
$$
という取り方である。

すべての$p(w)$がこのようにパラメタを取れるわけではないが、非常に多くの、そして確率モデルとして大事な場合にこのようなパラメタを取ることができることは知られており、そのような分布を、指数型分布族と呼ぶ。

そして、ここでは$F,H$という二つの行列を使って表していることからわかるように、$\theta$,$\eta$はそれぞれ「平坦」であるけれども、それを実現するための行列$F,H$はお互いに関係がない。

このようなときには、$\theta$,$\eta$座標系は直交していない。ただ、相互に平坦なだけである。

$H=F^{-1}$とすると、平坦であり、かつ、直交した２つの座標系にすることができる。

ちなみに、$F,H$などを使って、線形変換しても、平坦なままであることを、これらの接続が「アフィン接続」である、と呼ぶ理由である。


アフィン変換は、いわゆる行列による一次線形な変換に平行移動を加えたものであるが、ここで考えている$\alpha$を用いた関数が定義する接続は、アフィン接続であることが解っている。

$\alpha=\pm 1$の場合というのは、その中で特殊なものであって、「うまくパラメタを取れば平坦になる」ものである。

そして、アフィン接続なので、うまく変換することで、相互に直交なパラメタの取り方も存在することから、いつでも、平坦かつ直交な$\theta$,$\eta$の座標系のペアが取れる。

そして平坦ではないが、$\alpha=0$の場合というのは、2つの双対な座標系を取る、というこの仕組みの中で、「自己双対」である、という特殊な場合であること。

# 結論２

結局、尤度関数として我々が普通に使うものは、指数型分布族なので、必ず$\theta$,$\eta$座標系がとれて、それらは両方とも平坦で相互に直交している。ということが解った。

その背景には、異なる座標系での直線があり、異なる座標系にある二つの直線の長さ(KLd)をピタゴラスの定理的に計算できること、片方の座標系の部分空間へ、もう片方の座標系の垂線を引くことが簡単になること、などがあることもわかった。

そして、また、そこには、ユークリッド空間では、２つの座標系が、自己双対なために、１つの座標系で出来ていると見えること

# 球化処理による、ベクトルの長さの2乗と$\chi^2$統計量

mwaytableがあったときに、そのmwaytableの形($r_1\times r_2 \times ...$)に応じて、ある行列Rを作ることができて、それにより、このテーブルのセル数の長さのベクトルを変換することができる。

そのうちの１つの値は、全テーブルのセルの値の和が総数として固定していることから定数となり、結局、セル数より１小さい個数のパラメタに変換できる。


じつは、このパラメタは、情報幾何での$\eta$とよく似た性質を持っており、周辺度数を共有するテーブルは、周辺度数に対応するパラメタの値が等しくなる。

式で書けば
$$
\kappa = R p(w,\xi)
$$

と書けるが、
$$
\eta = F^{-1} p(w,\xi)
$$
と比べれば、$kappa$は$eta$の一次変換であることも解る。

実際、$\eta$座標系が平坦であるというときの接続($/alpha=-1$接続)はアフィン接続であり、線形変換しても相変わらず平坦であることから、一種の$\eta$座標系ともみなせる。

今、観察テーブルと期待値テーブルとがあって、その座標を$p_{alt}$,$p_{null}$としたとすると、尤度関数を作れば、そのKLdは$\chi^2$分布に従うことが知られている。

一方、この分割表の独立性の検定によって計算される$\chi^2$値が、以下のように計算できることを、球化手法は述べている。

球化した座標$\kappa_{alt}$,$\kappa_{null}$のうち、周辺度数に相当するパラメタの値は一致するので、それらは無視し、残りのパラメタ(自由度の数だけある)を取り出したものを$\kappa_{alt}'$,$\kappa_{null}'$と書くことにすれば、
その自由度次元空間での、2点のユークリッド距離の二乗が

$$
(\kappa_{alt}'-\kappa_{null}')^T K (\kappa_{alt}'-\kappa_{null}')
$$
と計算できるような、自由度ｘ自由度の正方行列$K$の算出方法がある、というのが、球化手法の行っていることである。

この正方行列$K$は、式の形から、『平坦な座標系である$\kappa$座標系の、部分多様体にある平坦な座標系$\kappa'$における、期待値表相当点での計量』である、と言っている。

結局、球化手法は、分割表における自由度サブ空間における、期待値表相当点でのフィッシャー情報量行列(リーマン計量)の算出法を定義した、と言える。

また、そのことから、局所における漸近近似の意味での、テーブルの期待値表からのずれを自由度次元標準正規分布にて行えることを可能にし、それを用いて、マルチプルテスティング補正も可能にすることなどの有用なモンテカルロ手法を可能とする。



